{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model import QwenLLM, RagEmbedding, RagLLM\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功，使用设备: cpu\n"
     ]
    }
   ],
   "source": [
    "embedding_cls = RagEmbedding(model_name=\"BAAI/bge-m3\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIRECTORY = \"./chroma_db/zhidu_db\"\n",
    "COLLECTION_NAME = \"zhidu_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhidu_db = Chroma(\n",
    "    COLLECTION_NAME,\n",
    "    embedding_cls,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    你是企业员工助手，熟悉公司考勤和报销标准等规章制度，需要根据提供的上下文信息context来回答员工的提问。\\\n",
    "    请直接回答问题，如果上下文信息context没有和问题相关的信息，请直接先回答不知道 \\\n",
    "    问题：{question} \n",
    "    \"{context}\"\n",
    "    回答：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = RagLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipline(query, context_query, k=3, context_query_type=\"query\", \n",
    "                          stream=False, prompt_template=prompt_template,\n",
    "                          temperature=0.1, llm=None, vector_db=None):\n",
    "    \"\"\"\n",
    "    修复版RAG管道函数\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        context_query (str or list): 用于检索的查询或文档\n",
    "        k (int): 返回的相关文档数量\n",
    "        context_query_type (str): 查询类型，可选值为\"query\"、\"vector\"或\"doc\"\n",
    "        stream (bool): 是否使用流式输出\n",
    "        prompt_template (str): 提示模板\n",
    "        temperature (float): 模型温度参数\n",
    "        llm (LangChain LLM): 语言模型实例，如果为None则创建新实例\n",
    "        vector_db: 向量数据库实例，如果为None则使用传入的文档\n",
    "        \n",
    "    Returns:\n",
    "        str: 模型回答\n",
    "    \"\"\"\n",
    "    # 创建LLM实例（如果未提供）\n",
    "    if llm is None:\n",
    "        print(\"创建新的QwenLLM实例...\")\n",
    "        llm = QwenLLM(timeout=60, max_retries=3)\n",
    "    \n",
    "    # 处理上下文检索\n",
    "    if context_query_type == \"doc\":\n",
    "        # 直接使用提供的文档\n",
    "        related_docs = context_query\n",
    "        context = \"\\n\".join([f\"上下文{i+1}: {doc} \\n\" for i, doc in enumerate(related_docs)])\n",
    "    else:\n",
    "        # 没有向量数据库时直接使用context_query作为上下文\n",
    "        if vector_db is None:\n",
    "            print(\"警告: 没有提供向量数据库，直接使用context_query作为上下文\")\n",
    "            if isinstance(context_query, list):\n",
    "                context = \"\\n\".join([f\"上下文{i+1}: {doc} \\n\" for i, doc in enumerate(context_query)])\n",
    "            else:\n",
    "                context = f\"上下文: {context_query}\"\n",
    "        else:\n",
    "            # 使用向量数据库检索\n",
    "            if context_query_type == \"vector\":\n",
    "                related_docs = vector_db.similarity_search_by_vector(context_query, k=k)\n",
    "            else:  # \"query\"\n",
    "                related_docs = vector_db.similarity_search(context_query, k=k)\n",
    "            \n",
    "            context = \"\\n\".join([f\"上下文{i+1}: {doc.page_content} \\n\" \n",
    "                              for i, doc in enumerate(related_docs)])\n",
    "    \n",
    "    # 打印调试信息\n",
    "    print()\n",
    "    print(\"#\"*100)\n",
    "    print(f\"query: {query}\")\n",
    "    print(f\"context: {context}\")\n",
    "    \n",
    "    # 构建提示\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=prompt_template,\n",
    "    )\n",
    "    llm_prompt = prompt.format(question=query, context=context)\n",
    "    \n",
    "    # 使用语言模型生成回答\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if stream:\n",
    "            print(f\"response: \")\n",
    "            response = llm(llm_prompt, stream=True, temperature=temperature)\n",
    "            full_response = \"\"\n",
    "            \n",
    "            try:\n",
    "                for chunk in response:\n",
    "                    if isinstance(chunk, dict) and 'choices' in chunk:\n",
    "                        text = chunk['choices'][0].get('text', '')\n",
    "                    elif hasattr(chunk, 'choices') and len(chunk.choices) > 0:\n",
    "                        text = chunk.choices[0].text\n",
    "                    else:\n",
    "                        text = str(chunk)\n",
    "                    \n",
    "                    print(text, end='', flush=True)\n",
    "                    full_response += text\n",
    "                \n",
    "                print()  # 添加换行\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"完成，耗时: {elapsed:.2f}秒\")\n",
    "                return full_response\n",
    "            except Exception as e:\n",
    "                print(f\"\\n流式输出处理错误: {str(e)}\")\n",
    "                # 失败时回退到非流式模式\n",
    "                return llm(llm_prompt, stream=False, temperature=temperature)\n",
    "        else:\n",
    "            # 非流式模式\n",
    "            response = llm(llm_prompt, stream=False, temperature=temperature)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"response: {response}\")\n",
    "            print(f\"完成，耗时: {elapsed:.2f}秒\")\n",
    "            return response\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"错误: {str(e)}\")\n",
    "        # 在发生错误时提供错误说明\n",
    "        return f\"抱歉，处理您的请求时发生错误: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query2doc\n",
    "- 利用大模型生成伪文档，来提升检索性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2doc(query):\n",
    "    prompt = f\"你是一名公司员工制度的问答助手, 熟悉公司规章制度，请简短回答以下问题: {query}\"\n",
    "    doc_info = llm(prompt, stream=False)\n",
    "    context_query = f\"{query}, {doc_info}\"\n",
    "    print(\"#\"*20, 'query2doc')\n",
    "    print(context_query)\n",
    "    print(\"#\"*20, 'query2doc')\n",
    "    return context_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"那个，我们公司有什么规定来着？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建新的QwenLLM实例...\n",
      "警告: 没有提供向量数据库，直接使用context_query作为上下文\n",
      "\n",
      "####################################################################################################\n",
      "query: 那个，我们公司有什么规定来着？\n",
      "context: 上下文: 那个，我们公司有什么规定来着？\n",
      "response: 公司的具体规定可能会包括考勤制度、休假政策、工作时间、绩效评价、报销标准以及相关的行为准则等。你需要查阅相关的员工手册或者询问人力资源部门以获取最准确的信息。\n",
      "完成，耗时: 1.97秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'公司的具体规定可能会包括考勤制度、休假政策、工作时间、绩效评价、报销标准以及相关的行为准则等。你需要查阅相关的员工手册或者询问人力资源部门以获取最准确的信息。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rag_pipline(query, query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### query2doc\n",
      "那个，我们公司有什么规定来着？, 抱歉，由于我是人工智能，并不具备实时查询具体公司内部规定的能力。我建议你可以查阅公司的员工手册或者联系人力资源部门获取准确的信息。\n",
      "#################### query2doc\n",
      "创建新的QwenLLM实例...\n",
      "警告: 没有提供向量数据库，直接使用context_query作为上下文\n",
      "\n",
      "####################################################################################################\n",
      "query: 那个，我们公司有什么规定来着？\n",
      "context: 上下文: 那个，我们公司有什么规定来着？, 抱歉，由于我是人工智能，并不具备实时查询具体公司内部规定的能力。我建议你可以查阅公司的员工手册或者联系人力资源部门获取准确的信息。\n",
      "response: 公司规定通常包含在员工手册中，内容可能包括考勤制度、休假政策、绩效评价、薪酬福利、保密协议和行为准则等。如果你需要具体某一方面的规定，建议直接咨询人力资源部门或查阅相关文件。\n",
      "完成，耗时: 2.31秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'公司规定通常包含在员工手册中，内容可能包括考勤制度、休假政策、绩效评价、薪酬福利、保密协议和行为准则等。如果你需要具体某一方面的规定，建议直接咨询人力资源部门或查阅相关文件。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rag_pipline(query, query2doc(query), k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
